{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "957cf823-6d7a-45d3-8d56-0580cab1ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "Stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1a3ea5-b6e2-4d63-8d27-9e6f698beca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_all_unique_words_and_freq(words):\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        # Jika kata sudah ada di dict, tambahkan 1. Jika belum, set nilainya jadi 1.\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    return word_freq\n",
    "    \n",
    "def finding_freq_of_word_in_doc(word,words):\n",
    "    \"\"\"Menghitung frekuensi sebuah kata dalam list kata-kata.\"\"\"\n",
    "    freq = words.count(word)\n",
    "    return freq\n",
    "        \n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\\\s]')\n",
    "    text_returned = re.sub(regex,'',text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a591d7c6-8bf7-482a-94a7-aa7372df4412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data\\1_1FMoK_HWvk1IBaMelUXibw.webp\n",
      "Processing: data\\badminton.txt\n",
      "Processing: data\\barack obama.txt\n",
      "Processing: data\\baseball.txt\n",
      "Processing: data\\lee quan yew.txt\n",
      "Processing: data\\narendra modi.txt\n",
      "Processing: data\\queen elizabeth.txt\n",
      "Processing: data\\shinzo abe.txt\n",
      "Processing: data\\table tennis.txt\n",
      "\n",
      "Processing finished.\n",
      "Total unique words found: 1356\n"
     ]
    }
   ],
   "source": [
    "# Import library yang dibutuhkan\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter # <-- Gunakan Counter untuk efisiensi\n",
    "\n",
    "# Asumsi Anda punya variabel Stopwords dan fungsi-fungsi lainnya dari cell sebelumnya\n",
    "# Stopwords = set(stopwords.words('english'))\n",
    "# def remove_special_characters(text): ...\n",
    "\n",
    "# --- Kode Utama yang Diperbaiki ---\n",
    "\n",
    "dict_global = {}\n",
    "file_folder = 'data/*'\n",
    "idx = 0\n",
    "files_with_index = {}\n",
    "\n",
    "# Gunakan 'fname' agar tidak bentrok dengan variabel 'file'\n",
    "for fname in glob.glob(file_folder):\n",
    "    print(f\"Processing: {fname}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Gunakan 'with' untuk membuka file secara aman\n",
    "        # 2. Tambahkan parameter encoding=\"utf-8\"\n",
    "        with open(fname, \"r\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        text = remove_special_characters(text)\n",
    "        text = re.sub(re.compile('\\\\d'),'',text)\n",
    "        \n",
    "        # sentences = sent_tokenize(text) # Variabel ini tidak digunakan\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # 3. Perbaiki logika: harusnya len(word) bukan len(words)\n",
    "        words = [word for word in words if len(word) > 1]\n",
    "        \n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word for word in words if word not in Stopwords]\n",
    "        \n",
    "        # 4. (SANGAT DISARANKAN) Gunakan Counter yang jauh lebih cepat\n",
    "        #    daripada fungsi finding_all_unique_words_and_freq Anda yang lama.\n",
    "        word_freq = Counter(words)\n",
    "        dict_global.update(word_freq)\n",
    "        \n",
    "        files_with_index[idx] = os.path.basename(fname)\n",
    "        idx += 1 # Cara penulisan yang lebih umum untuk increment\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"--> SKIPPING FILE due to encoding error: {fname}\")\n",
    "    except Exception as e:\n",
    "        print(f\"--> An unexpected error occurred with {fname}: {e}\")\n",
    "        \n",
    "unique_words_all = set(dict_global.keys())\n",
    "\n",
    "print(\"\\nProcessing finished.\")\n",
    "print(f\"Total unique words found: {len(unique_words_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23cd8af2-a79c-4b8f-a517-040a68e5f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self ,docId, freq = None):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.nextval = None\n",
    "    \n",
    "class SlinkedList:\n",
    "    def __init__(self ,head = None):\n",
    "        self.head = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aab048d-d680-4acc-ad59-a673bc12912c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'g'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m word_freq_in_doc = finding_all_unique_words_and_freq(words)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_freq_in_doc.keys():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     linked_list = \u001b[43mlinked_list_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m.head\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m linked_list.nextval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     21\u001b[39m         linked_list = linked_list.nextval\n",
      "\u001b[31mKeyError\u001b[39m: 'g'"
     ]
    }
   ],
   "source": [
    "linked_list_data = {}\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = SlinkedList()\n",
    "    linked_list_data[word].head = Node(1,Node)\n",
    "word_freq_in_doc = {}\n",
    "idx = 1\n",
    "for file in glob.glob(file_folder):\n",
    "    file = open(file, \"r\", encoding=\"latin-1\")\n",
    "    text = file.read()\n",
    "    text = remove_special_characters(text)\n",
    "    text = re.sub(re.compile('\\\\d'),'',text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if len(words)>1]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in Stopwords]\n",
    "    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "    for word in word_freq_in_doc.keys():\n",
    "        linked_list = linked_list_data[word].head\n",
    "        while linked_list.nextval is not None:\n",
    "            linked_list = linked_list.nextval\n",
    "        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cc2ae-550a-42d5-84bf-0932595deb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
